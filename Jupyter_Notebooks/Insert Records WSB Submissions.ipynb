{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "578004c7-d6b4-47b6-b5d6-f9a05f7fbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0855bb57-98f2-4dd9-9b51-d76fd846e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install psycopg2\n",
    "\n",
    "import psycopg2\n",
    "import time\n",
    "import boto3\n",
    "import copy\n",
    "import gc\n",
    "import pandas as pd\n",
    "import json\n",
    "import ujson\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd2ab20-5b46-45c4-8015-e452fb1a183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = psycopg2.connect(\n",
    "            user='postgres', password='Data2023',\n",
    "            host='mads-capstone.cmohac77hep9.eu-north-1.rds.amazonaws.com', port=5432,\n",
    "            database=\"mads\"\n",
    "        )\n",
    "\n",
    "db_connection.autocommit = True\n",
    "\n",
    "cursor = db_connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3d7023e9-35ef-49c8-a1da-51a65357d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cursor.execute(\"DROP TABLE submissions;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "313c776b-fbb5-4a41-aff8-d43dfefcd575",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''CREATE TABLE submissions (  \n",
    "                                id VARCHAR(16),\n",
    "                                subreddit_id VARCHAR(24),\n",
    "                                subreddit text,\n",
    "                                author VARCHAR(50),\n",
    "                                created_utc INT,\n",
    "                                permalink text,\n",
    "                                title text,\n",
    "                                selftext text,\n",
    "                                num_comments INT,\n",
    "                                score INT)'''\n",
    "\n",
    "\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "3499e355-11ab-4898-b880-d337abeafbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,)]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "select_query = \"SELECT count(*) FROM submissions\"\n",
    "\n",
    "cursor.execute(select_query)\n",
    "\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24058520-8ae9-4e71-804a-a510015973e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(row: str)-> dict:\n",
    "    try:\n",
    "        row_data = row.decode(encoding=\"utf-8\", errors=\"replace\").\\\n",
    "                                            replace(\"null\", '\"\"').\\\n",
    "                                            replace(\"&lt;\", \"<\").\\\n",
    "                                            replace(\"&gt;\", \">\").\\\n",
    "                                            replace(\"&amp;#39;\", \"\").\\\n",
    "                                            replace(\"&amp;\", \"\").\\\n",
    "                                            replace(\"false\", \"False\").\\\n",
    "                                            replace(\"true\", \"True\")\n",
    "                                            #replace(\",\", \";\")\n",
    "        #print(row_data)\n",
    "        row_dict = eval(row_data)\n",
    "        row_dict[\"selftext\"] = row_dict[\"selftext\"].replace(\",\", \";\").replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"\\t\", \"\").replace(\"\\\\\", \"\") if row_dict[\"selftext\"] else \"\"\n",
    "        row_dict[\"title\"] = row_dict[\"title\"].replace(\",\", \";\").replace(\"\\n\", \" \").replace(\"\\r\", \"\").replace(\"\\t\", \"\").replace(\"\\\\\", \"\") if row_dict[\"title\"] else \"\"\n",
    "        return row_dict\n",
    "    except SyntaxError as e: ## Ignore syntax error records\n",
    "        #print(e)\n",
    "        pass\n",
    "        #print(row_data)\n",
    "    except UnicodeEncodeError as e: ## Ignore Decode error records\n",
    "        #print(e)\n",
    "        pass\n",
    "    except Exception as e: ## Ignore all other errors for continuous processing.\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "def save_dataframe(records):\n",
    "    df = pd.DataFrame(records)\n",
    "    #print(df.columns)\n",
    "    df = df[selected_columns]\n",
    "    df.to_csv(\"submissions.csv\", index=False, header=False, encoding=\"utf-8\", errors=\"replace\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc158406-c96e-4907-80e4-109842cf3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'mads-capstone-2023' \n",
    "data_key = 'wallstreetbets_submissions.json' \n",
    "\n",
    "selected_columns = [\"id\", \"subreddit_id\", \"subreddit\", \"author\", \"created_utc\", \"permalink\", \"title\", \"selftext\", \"num_comments\", \"score\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2dab8-2b35-48cd-aefe-3e71e1b11e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=data_key)\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for i, row in enumerate(obj[\"Body\"].iter_lines()):\n",
    "    cnt +=1\n",
    "    if (i%1_000_000)==0:\n",
    "        print(f\"{i}\")\n",
    "        \n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "554b4de8-1f3c-4943-8a7c-e9162aaa4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = 2218243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "cac520ab-0bd4-4a9d-8dea-74f84f57a9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to escape, but no escapechar set\n",
      "Records 500000-600000 Failed\n",
      "1000000 Records Processed\n",
      "need to escape, but no escapechar set\n",
      "Records 1900000-2000000 Failed\n",
      "2000000 Records Processed\n",
      "need to escape, but no escapechar set\n",
      "Records 2000000-2100000 Failed\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=data_key)\n",
    "\n",
    "records = []\n",
    "failed_seqs = []\n",
    "\n",
    "for i, row in enumerate(obj[\"Body\"].iter_lines()):\n",
    "    parsed_record = parse_record(row)\n",
    "    if parsed_record:\n",
    "        records.append(parsed_record)\n",
    "    \n",
    "    if i>0 and (i%100_000)==0:\n",
    "        try:\n",
    "            save_dataframe(records)\n",
    "            with open(\"submissions.csv\", encoding=\"utf-8\", errors=\"replace\") as fp:\n",
    "                cursor.copy_from(fp, \"submissions\", sep=\",\", null=\"\", columns=selected_columns)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Records {i-100_000}-{i} Failed\")\n",
    "            failed_seqs.append((i-100_000, i))\n",
    "\n",
    "        records = []\n",
    "        gc.collect();\n",
    "        if (i%1_000_000)==0:\n",
    "            print(f'{i} Records Processed')\n",
    "\n",
    "if records:\n",
    "    try:\n",
    "        save_dataframe(records)\n",
    "        with open(\"submissions.csv\") as fp:\n",
    "            cursor.copy_from(fp, \"submissions\", sep=\",\", null=\"\", columns=selected_columns)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        #print(f\"Records {i-100}-{i} Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9047a330-8bb2-47c7-a528-71c1190d5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#failed_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6da18d11-36eb-4eb3-b0c5-0877d6d3e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat submissions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd642016-af74-44c4-95d6-9107071cad25",
   "metadata": {},
   "source": [
    "## Logic to Insert Missing Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35bfa4a-3b64-42a5-a12b-25980f5deb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to escape, but no escapechar set\n",
      "Records 549000-550000 Failed\n",
      "550000 Records Processed\n",
      "560000 Records Processed\n",
      "570000 Records Processed\n",
      "580000 Records Processed\n",
      "590000 Records Processed\n",
      "600000 Records Processed\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=data_key)\n",
    "\n",
    "records = []\n",
    "failed_seqs = []\n",
    "start, end, batch_size = 500000, 600000, 1000\n",
    "\n",
    "for i, row in enumerate(obj[\"Body\"].iter_lines()):\n",
    "    if i>start and i<=end:\n",
    "        parsed_record = parse_record(row)\n",
    "        if parsed_record:\n",
    "            records.append(parsed_record)\n",
    "\n",
    "        if i>0 and (i%batch_size)==0:\n",
    "            try:\n",
    "                save_dataframe(records)\n",
    "                with open(\"submissions.csv\", encoding=\"utf-8\", errors=\"replace\") as fp:\n",
    "                    cursor.copy_from(fp, \"submissions\", sep=\",\", null=\"\", columns=selected_columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Records {i-batch_size}-{i} Failed\")\n",
    "                os.rename('submissions.csv', 'submissions_{}_{}.csv'.format(i-batch_size, i))\n",
    "                failed_seqs.append((i-batch_size, i))\n",
    "\n",
    "            records = []\n",
    "            gc.collect();\n",
    "            if (i%(batch_size*10))==0:\n",
    "                print(f'{i} Records Processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f22e351-c712-4258-b8c1-b4cea6152746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1910000 Records Processed\n",
      "1920000 Records Processed\n",
      "1930000 Records Processed\n",
      "1940000 Records Processed\n",
      "1950000 Records Processed\n",
      "1960000 Records Processed\n",
      "1970000 Records Processed\n",
      "1980000 Records Processed\n",
      "1990000 Records Processed\n",
      "need to escape, but no escapechar set\n",
      "Records 1998000-1999000 Failed\n",
      "2000000 Records Processed\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=data_key)\n",
    "\n",
    "records = []\n",
    "failed_seqs = []\n",
    "start, end, batch_size = 1900000, 2000000, 1000\n",
    "\n",
    "for i, row in enumerate(obj[\"Body\"].iter_lines()):\n",
    "    if i>start and i<=end:\n",
    "        parsed_record = parse_record(row)\n",
    "        if parsed_record:\n",
    "            records.append(parsed_record)\n",
    "\n",
    "        if i>0 and (i%batch_size)==0:\n",
    "            try:\n",
    "                save_dataframe(records)\n",
    "                with open(\"submissions.csv\", encoding=\"utf-8\", errors=\"replace\") as fp:\n",
    "                    cursor.copy_from(fp, \"submissions\", sep=\",\", null=\"\", columns=selected_columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Records {i-batch_size}-{i} Failed\")\n",
    "                os.rename('submissions.csv', 'submissions_{}_{}.csv'.format(i-batch_size, i))\n",
    "                failed_seqs.append((i-batch_size, i))\n",
    "\n",
    "            records = []\n",
    "            gc.collect();\n",
    "            if (i%(batch_size*10))==0:\n",
    "                print(f'{i} Records Processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e42611ce-2381-40e3-8905-29377c1d0a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010000 Records Processed\n",
      "2020000 Records Processed\n",
      "need to escape, but no escapechar set\n",
      "Records 2022000-2023000 Failed\n",
      "2030000 Records Processed\n",
      "2040000 Records Processed\n",
      "need to escape, but no escapechar set\n",
      "Records 2049000-2050000 Failed\n",
      "2050000 Records Processed\n",
      "2060000 Records Processed\n",
      "2070000 Records Processed\n",
      "2080000 Records Processed\n",
      "2090000 Records Processed\n",
      "2100000 Records Processed\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=data_key)\n",
    "\n",
    "records = []\n",
    "failed_seqs = []\n",
    "start, end, batch_size = 2000000, 2100000, 1000\n",
    "\n",
    "for i, row in enumerate(obj[\"Body\"].iter_lines()):\n",
    "    if i>start and i<=end:\n",
    "        parsed_record = parse_record(row)\n",
    "        if parsed_record:\n",
    "            records.append(parsed_record)\n",
    "\n",
    "        if i>0 and (i%batch_size)==0:\n",
    "            try:\n",
    "                save_dataframe(records)\n",
    "                with open(\"submissions.csv\", encoding=\"utf-8\", errors=\"replace\") as fp:\n",
    "                    cursor.copy_from(fp, \"submissions\", sep=\",\", null=\"\", columns=selected_columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Records {i-batch_size}-{i} Failed\")\n",
    "                os.rename('submissions.csv', 'submissions_{}_{}.csv'.format(i-batch_size, i))\n",
    "                failed_seqs.append((i-batch_size, i))\n",
    "\n",
    "            records = []\n",
    "            gc.collect();\n",
    "            if (i%(batch_size*10))==0:\n",
    "                print(f'{i} Records Processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1be8be8-d4ea-41e2-9ae9-46c95f12169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049100 Records Processed\n",
      "2049200 Records Processed\n",
      "2049300 Records Processed\n",
      "2049400 Records Processed\n",
      "2049500 Records Processed\n",
      "2049600 Records Processed\n",
      "need to escape, but no escapechar set\n",
      "Records 2049660-2049670 Failed\n",
      "need to escape, but no escapechar set\n",
      "Records 2049670-2049680 Failed\n",
      "2049700 Records Processed\n",
      "2049800 Records Processed\n",
      "2049900 Records Processed\n",
      "2050000 Records Processed\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=data_key)\n",
    "\n",
    "records = []\n",
    "failed_seqs = []\n",
    "start, end, batch_size = 2049000, 2050000, 10\n",
    "\n",
    "for i, row in enumerate(obj[\"Body\"].iter_lines()):\n",
    "    if i>start and i<=end:\n",
    "        parsed_record = parse_record(row)\n",
    "        if parsed_record:\n",
    "            records.append(parsed_record)\n",
    "\n",
    "        if i>0 and (i%batch_size)==0:\n",
    "            try:\n",
    "                save_dataframe(records)\n",
    "                with open(\"submissions.csv\", encoding=\"utf-8\", errors=\"replace\") as fp:\n",
    "                    cursor.copy_from(fp, \"submissions\", sep=\",\", null=\"\", columns=selected_columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Records {i-batch_size}-{i} Failed\")\n",
    "                os.rename('submissions.csv', 'submissions_{}_{}.csv'.format(i-batch_size, i))\n",
    "                failed_seqs.append((i-batch_size, i))\n",
    "\n",
    "            records = []\n",
    "            gc.collect();\n",
    "            if (i%(batch_size*10))==0:\n",
    "                print(f'{i} Records Processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c3886-96a0-40fc-b67b-ed50fa3bc3e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Checking Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a4646b0-d7da-4b84-9e15-7e5b2ec00fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2218139,)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "select_query = \"SELECT count(*) FROM submissions\"\n",
    "\n",
    "cursor.execute(select_query)\n",
    "\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d2c30-8979-4d80-af3a-fb961977a865",
   "metadata": {},
   "source": [
    "## Creating 2nd Submissions Table and Insert Records In It."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "816c9454-e073-4c89-b1cd-229dd7ea857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''CREATE TABLE submissions2 (  \n",
    "                                id VARCHAR(16),\n",
    "                                subreddit_id VARCHAR(24),\n",
    "                                subreddit text,\n",
    "                                author VARCHAR(50),\n",
    "                                created_utc INT,\n",
    "                                permalink text,\n",
    "                                title text,\n",
    "                                selftext text,\n",
    "                                num_comments INT,\n",
    "                                score INT)'''\n",
    "\n",
    "\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b554cb3f-47b8-49cc-a66b-2ab4ace556c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to escape, but no escapechar set\n",
      "Records 549000-550000 Failed\n",
      "1000000 Records Processed\n",
      "need to escape, but no escapechar set\n",
      "Records 1998000-1999000 Failed\n",
      "2000000 Records Processed\n",
      "need to escape, but no escapechar set\n",
      "Records 2022000-2023000 Failed\n",
      "need to escape, but no escapechar set\n",
      "Records 2049000-2050000 Failed\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=data_key)\n",
    "\n",
    "records = []\n",
    "failed_seqs = []\n",
    "batch_size = 1000\n",
    "\n",
    "for i, row in enumerate(obj[\"Body\"].iter_lines()):\n",
    "    parsed_record = parse_record(row)\n",
    "    if parsed_record:\n",
    "        records.append(parsed_record)\n",
    "    \n",
    "    if i>0 and (i%batch_size)==0:\n",
    "        try:\n",
    "            save_dataframe(records)\n",
    "            with open(\"submissions.csv\", encoding=\"utf-8\", errors=\"replace\") as fp:\n",
    "                cursor.copy_from(fp, \"submissions2\", sep=\",\", null=\"\", columns=selected_columns)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Records {i-batch_size}-{i} Failed\")\n",
    "            failed_seqs.append((i-batch_size, i))\n",
    "\n",
    "        records = []\n",
    "        gc.collect();\n",
    "        if (i%1_000_000)==0:\n",
    "            print(f'{i} Records Processed')\n",
    "\n",
    "if records:\n",
    "    try:\n",
    "        save_dataframe(records)\n",
    "        with open(\"submissions.csv\") as fp:\n",
    "            cursor.copy_from(fp, \"submissions2\", sep=\",\", null=\"\", columns=selected_columns)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        #print(f\"Records {i-100}-{i} Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac6a2bd9-d123-41ad-9da2-ddd0e532e92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049100 Records Processed\n",
      "2049200 Records Processed\n",
      "2049300 Records Processed\n",
      "2049400 Records Processed\n",
      "2049500 Records Processed\n",
      "2049600 Records Processed\n",
      "need to escape, but no escapechar set\n",
      "Records 2049660-2049670 Failed\n",
      "need to escape, but no escapechar set\n",
      "Records 2049670-2049680 Failed\n",
      "2049700 Records Processed\n",
      "2049800 Records Processed\n",
      "2049900 Records Processed\n",
      "2050000 Records Processed\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=data_key)\n",
    "\n",
    "records = []\n",
    "failed_seqs = []\n",
    "start, end, batch_size = 2049000, 2050000, 10\n",
    "\n",
    "for i, row in enumerate(obj[\"Body\"].iter_lines()):\n",
    "    if i>start and i<=end:\n",
    "        parsed_record = parse_record(row)\n",
    "        if parsed_record:\n",
    "            records.append(parsed_record)\n",
    "\n",
    "        if i>0 and (i%batch_size)==0:\n",
    "            try:\n",
    "                save_dataframe(records)\n",
    "                with open(\"submissions.csv\", encoding=\"utf-8\", errors=\"replace\") as fp:\n",
    "                    cursor.copy_from(fp, \"submissions2\", sep=\",\", null=\"\", columns=selected_columns)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Records {i-batch_size}-{i} Failed\")\n",
    "                os.rename('submissions.csv', 'submissions_{}_{}.csv'.format(i-batch_size, i))\n",
    "                failed_seqs.append((i-batch_size, i))\n",
    "\n",
    "            records = []\n",
    "            gc.collect();\n",
    "            if (i%(batch_size*10))==0:\n",
    "                print(f'{i} Records Processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf9fc456-9ad2-4164-be8a-33816fe76717",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_query = \"CREATE INDEX submissions2_date ON submissions2(created_utc);\"\n",
    "\n",
    "cursor.execute(index_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f383357-2d6b-4176-bb91-3654fddea5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2218139,)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "select_query = \"SELECT count(*) FROM submissions2\"\n",
    "\n",
    "cursor.execute(select_query)\n",
    "\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d4a2a-160e-458d-95ee-995b3a23a781",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aee0897-8942-4390-a789-2c9a1f1b1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_query = \"CREATE INDEX submissions_date ON submissions(created_utc);\"\n",
    "\n",
    "cursor.execute(index_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "eb3d7bf6-9987-400a-909c-bf3c81aaedcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s4jw1</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1334162440</td>\n",
       "      <td>/r/wallstreetbets/comments/s4jw1/earnings_seas...</td>\n",
       "      <td>Earnings season is here.  Place your bets.</td>\n",
       "      <td>I know that /r/investing is a great place for ...</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s6r57</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1334263051</td>\n",
       "      <td>/r/wallstreetbets/comments/s6r57/goog_beat_est...</td>\n",
       "      <td>GOOG - beat estimates; price barely rises.</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sd5ai</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1334615377</td>\n",
       "      <td>/r/wallstreetbets/comments/sd5ai/my_poorly_tim...</td>\n",
       "      <td>My poorly timed opening position for AAPL earn...</td>\n",
       "      <td>So I missed out on GOOG; which is probably a g...</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>se66f</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1334670090</td>\n",
       "      <td>/r/wallstreetbets/comments/se66f/anyone_bettin...</td>\n",
       "      <td>Anyone betting on VVUS and their potential app...</td>\n",
       "      <td>\"I'm normally a long; but I've created a secon...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sinsk</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>CheeseYogi</td>\n",
       "      <td>1334877676</td>\n",
       "      <td>/r/wallstreetbets/comments/sinsk/after_hgsi_sp...</td>\n",
       "      <td>After HGSI spikes 97%; will share price drop a...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id subreddit_id       subreddit      author  created_utc  \\\n",
       "0  s4jw1     t5_2th52  wallstreetbets   [deleted]   1334162440   \n",
       "1  s6r57     t5_2th52  wallstreetbets   [deleted]   1334263051   \n",
       "2  sd5ai     t5_2th52  wallstreetbets   [deleted]   1334615377   \n",
       "3  se66f     t5_2th52  wallstreetbets   [deleted]   1334670090   \n",
       "4  sinsk     t5_2th52  wallstreetbets  CheeseYogi   1334877676   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  /r/wallstreetbets/comments/s4jw1/earnings_seas...   \n",
       "1  /r/wallstreetbets/comments/s6r57/goog_beat_est...   \n",
       "2  /r/wallstreetbets/comments/sd5ai/my_poorly_tim...   \n",
       "3  /r/wallstreetbets/comments/se66f/anyone_bettin...   \n",
       "4  /r/wallstreetbets/comments/sinsk/after_hgsi_sp...   \n",
       "\n",
       "                                               title  \\\n",
       "0         Earnings season is here.  Place your bets.   \n",
       "1         GOOG - beat estimates; price barely rises.   \n",
       "2  My poorly timed opening position for AAPL earn...   \n",
       "3  Anyone betting on VVUS and their potential app...   \n",
       "4  After HGSI spikes 97%; will share price drop a...   \n",
       "\n",
       "                                            selftext  num_comments  score  \n",
       "0  I know that /r/investing is a great place for ...            22     13  \n",
       "1                                               None             0      2  \n",
       "2  So I missed out on GOOG; which is probably a g...            21     12  \n",
       "3  \"I'm normally a long; but I've created a secon...             0      1  \n",
       "4                                               None             0      4  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_query = \"SELECT * FROM submissions LIMIT 100;\"\n",
    "\n",
    "cursor.execute(select_query)\n",
    "\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "records = []\n",
    "for row in rows:\n",
    "    records.append([row[0], row[1],row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[9]])\n",
    "    \n",
    "df = pd.DataFrame(records, columns=selected_columns)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58e651d1-d8df-4c8b-93e1-0dba30d61de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cols : 35\n",
      "\n",
      "['approved_by', 'author', 'author_flair_css_class', 'author_flair_text', 'banned_by', 'clicked', 'created', 'created_utc', 'distinguished', 'domain', 'downs', 'edited', 'hidden', 'id', 'is_self', 'likes', 'link_flair_css_class', 'link_flair_text', 'media', 'media_embed', 'name', 'num_comments', 'num_reports', 'over_18', 'permalink', 'saved', 'score', 'selftext', 'selftext_html', 'subreddit', 'subreddit_id', 'thumbnail', 'title', 'ups', 'url']\n"
     ]
    }
   ],
   "source": [
    "sorted_cols = sorted(df.columns.values) ### Read first 100 Rows\n",
    "\n",
    "print(\"Total Cols : {}\\n\".format(len(sorted_cols)))\n",
    "print(sorted_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "617797fe-2831-46fd-843e-6ad6696e37f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'downs': 2,\n",
       " 'link_flair_text': '',\n",
       " 'distinguished': '',\n",
       " 'media': '',\n",
       " 'url': 'http://www.reddit.com/r/wallstreetbets/comments/sd5ai/my_poorly_timed_opening_position_for_aapl/',\n",
       " 'link_flair_css_class': '',\n",
       " 'id': 'sd5ai',\n",
       " 'edited': True,\n",
       " 'num_reports': '',\n",
       " 'created_utc': 1334615377,\n",
       " 'banned_by': '',\n",
       " 'name': 't3_sd5ai',\n",
       " 'subreddit': 'wallstreetbets',\n",
       " 'title': 'My poorly timed opening position for AAPL earnings bet',\n",
       " 'author_flair_text': '',\n",
       " 'is_self': True,\n",
       " 'author': '[deleted]',\n",
       " 'media_embed': {},\n",
       " 'permalink': '/r/wallstreetbets/comments/sd5ai/my_poorly_timed_opening_position_for_aapl/',\n",
       " 'author_flair_css_class': '',\n",
       " 'selftext': \"So I missed out on GOOG, which is probably a good thing given their split/dividend announcement really put a damper on things.  But I'm not planning on missing out on AAPL and today was a perfect day to lock in a bet.  Unfortunately for me I got greedy and subsequently got slapped on the wrist.\\n\\nPosition: Bought an OTM straddle with intentions on selling right before earnings.  This way I don't care about the results and ride the volatility wave up to the wave's crest.\\n\\n* BTO AAPL May put @ 525 for $10.10\\n* BTO AAPL May call @ 665 $7.60\\n\\nIt was poorly timed, because I tried to time the swings today hoping to trap some profits inside of the straddle (instead of buying simultaneously).  I bought the call during a price dip and I waited for prices to go back up before buying the put, which never really happened.  That said, I [ended up ok](http://i.imgur.com/390Bv.png).  \\n\\nAny other AAPL takers?\",\n",
       " 'domain': 'self.wallstreetbets',\n",
       " 'num_comments': 21,\n",
       " 'likes': '',\n",
       " 'clicked': False,\n",
       " 'thumbnail': 'default',\n",
       " 'saved': False,\n",
       " 'subreddit_id': 't5_2th52',\n",
       " 'ups': 14,\n",
       " 'approved_by': '',\n",
       " 'score': 12,\n",
       " 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>So I missed out on GOOG, which is probably a good thing given their split/dividend announcement really put a damper on things.  But Im not planning on missing out on AAPL and today was a perfect day to lock in a bet.  Unfortunately for me I got greedy and subsequently got slapped on the wrist.</p>\\n\\n<p>Position: Bought an OTM straddle with intentions on selling right before earnings.  This way I dont care about the results and ride the volatility wave up to the waves crest.</p>\\n\\n<ul>\\n<li>BTO AAPL May put @ 525 for $10.10</li>\\n<li>BTO AAPL May call @ 665 $7.60</li>\\n</ul>\\n\\n<p>It was poorly timed, because I tried to time the swings today hoping to trap some profits inside of the straddle (instead of buying simultaneously).  I bought the call during a price dip and I waited for prices to go back up before buying the put, which never really happened.  That said, I <a href=\"http://i.imgur.com/390Bv.png\">ended up ok</a>.  </p>\\n\\n<p>Any other AAPL takers?</p>\\n</div><!-- SC_ON -->',\n",
       " 'created': 1334618977.0,\n",
       " 'hidden': False,\n",
       " 'over_18': False,\n",
       " 'secure_media_embed': nan,\n",
       " 'secure_media': nan,\n",
       " 'stickied': nan,\n",
       " 'retrieved_on': nan,\n",
       " 'gilded': nan,\n",
       " 'report_reasons': nan,\n",
       " 'user_reports': nan,\n",
       " 'mod_reports': nan,\n",
       " 'hide_score': nan,\n",
       " 'from': nan,\n",
       " 'archived': nan,\n",
       " 'quarantine': nan,\n",
       " 'from_kind': nan,\n",
       " 'from_id': nan,\n",
       " 'preview': nan,\n",
       " 'post_hint': nan,\n",
       " 'locked': nan,\n",
       " 'contest_mode': nan,\n",
       " 'spoiler': nan}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = df.to_dict(orient=\"records\")\n",
    "\n",
    "rows[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcddf361-2614-470b-b415-d9287380f412",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Some Common Attributes Which Can Be Useful\n",
    "\n",
    "1. id\n",
    "2. subreddit_id (unique id, it is referred to by comments.json with parent_id)\n",
    "3. title\n",
    "4. selftext\n",
    "5. selftext_html (Same as previous one)\n",
    "6. num_comments\n",
    "7. ups\n",
    "8. downs\n",
    "9. created_utc (if time needed)\n",
    "10. score (This one is (ups - down), so it can be avoided i guess)\n",
    "11. author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56258c60-8067-4fe6-940a-80c29ff9a9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-north-1:243637512696:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
