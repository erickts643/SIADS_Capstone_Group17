{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b526d6-ec19-4147-b183-af26c2bea137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864e623a-f9a3-4251-bddd-8d6006b84534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import praw\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50547b23-50ce-4741-9172-073497ae1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    def preprocess_text(text):\n",
    "        \n",
    "        # Remove specific unwanted characters\n",
    "        text = re.sub(r'[^A-Za-z0-9\\s,.!?;:()\\'\\\"-]', '', text)\n",
    "        \n",
    "        # Strip whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        df['title'] = df['title'].fillna('')\n",
    "        df['title'] = df['title'].apply(preprocess_text)\n",
    "\n",
    "        # Create a new column with shifted values\n",
    "        df['title_prior'] = df['title'].shift(1)\n",
    "        # Drop rows where the value in 'ColumnToCheck' is the same as in 'ShiftedColumn'\n",
    "        df = df[df['title'] != df['title_prior']]\n",
    "        # drop the 'ShiftedColumn'\n",
    "        df = df.drop('title_prior', axis=1)\n",
    "        \n",
    "    except:\n",
    "        print(f'No title found, skipping')\n",
    "    \n",
    "    \n",
    "    # handle blank \n",
    "    df['selftext'] = df['selftext'].fillna('') \n",
    "    \n",
    "    # preprocess selftext\n",
    "    df['selftext'] = df['selftext'].apply(preprocess_text)\n",
    "    \n",
    "    # localize the UTC time stamp\n",
    "    df['created_EST_date'] = pd.to_datetime(df['created_utc'], unit='s').dt.tz_localize('UTC').dt.tz_convert('America/New_York').dt.date\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8522d8e-b7bf-4803-87a3-0a5e1c51a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tickers(df):\n",
    " \n",
    "    # Load tickers from a CSV file\n",
    "    stocks = pd.read_csv('r/root/Git_Repo/SIADS_Capstone_Group17/Data/nasdaq_screener.csv')\n",
    "\n",
    "    # Directly convert the 'Symbol' column to a set\n",
    "    tickers_set = set(stocks['Symbol'])\n",
    "\n",
    "    # Use set union to add additional tickers\n",
    "    additional_tickers = {'BBBY'}\n",
    "    tickers_set = tickers_set.union(additional_tickers)\n",
    "    \n",
    "    # Adjust the pattern to optionally include a leading '$'\n",
    "    # pattern = r'\\b\\$?(?:' + '|'.join(tickers_as_strings) + r')\\b'\n",
    "    # pattern = r'\\b\\$?(?:\\(?)(?:' + '|'.join(tickers_as_strings) + r')(?:\\)?)\\b'\n",
    "    pattern = r'\\b[A-Z]{2,5}\\b'\n",
    "    compiled_pattern = re.compile(pattern)\n",
    "    \n",
    "    \n",
    "    \n",
    "    blacklist = {\n",
    "#                 {'I', 'ELON', 'WSB', 'THE', 'A', 'ROPE', 'YOLO', 'TOS', 'CEO', 'DD', 'IT', 'OPEN', 'ATH', 'PM', 'IRS', 'FOR',\n",
    "#              'DEC', 'BE',\n",
    "                'IMO',# 'ALL', 'RH', 'EV', 'TOS', 'CFO', 'CTO',\n",
    "                'DD',\n",
    "                #'BTFD', 'WSB', 'OK', 'PDT', 'RH', 'KYS', 'FD',\n",
    "#              'TYS', \n",
    "                'US',\n",
    "                'USA',\n",
    "                # 'IT', 'ATH', 'RIP', 'BMW', 'GDP', 'OTM', 'ATM', 'ITM', \n",
    "                'IMO', 'LOL', 'AM', 'BE', 'PR', 'PRAY',\n",
    "#              'PT', 'FBI', 'SEC', 'GOD', 'NOT', 'POS', 'FOMO', 'TL;DR',\n",
    "                'EDIT', 'STILL', 'WTF', 'RAW', 'PM', 'LMAO', 'LMFAO',\n",
    "#              'ROFL', 'EZ', 'RED', 'BEZOS', 'TICK', 'IS', 'PM', 'LPT', 'GOAT', 'FL', 'CA', 'IL', 'MACD', 'HQ', 'OP', 'PS', 'AH',\n",
    "#              'TL', 'JAN', 'FEB', 'JUL', 'AUG', 'SEP', 'SEPT', 'OCT', 'NOV', 'FDA', 'IV', 'ER', 'IPO', 'MILF', 'BUT', 'SSN', 'FIFA',\n",
    "#              'USD', 'CPU', 'AT', 'GG', 'Mar', \n",
    "            \n",
    "#                # Jake added\n",
    "                'RUN', # common\n",
    "                'SAY', # common\n",
    "                'EOD', # end of day\n",
    "                'BIG', # common\n",
    "                'LOW', # low / high\n",
    "                'RSI', #relative strenght\n",
    "                'DT', #double top\n",
    "                'HUGE',\n",
    "                'U', # you\n",
    "                'AI', # Artificial Intelligence\n",
    "                'DC', # washington DC\n",
    "                'J', # as in J Powell\n",
    "                'ES', # E-mini SP future\n",
    "                'F', # f*ck\n",
    "                'GO',\n",
    "                'UK', # United Kingdom\n",
    "                'EU', # european union\n",
    "                'RH', # Robinhood, not Restoration Hardware\n",
    "                'E', # E*trade brokerage\n",
    "                'L', # L for loss, P&L etc\n",
    "                'R', # common \n",
    "                'K', # OK\n",
    "                'B', # common in BBBY odd spacing (spam?)\n",
    "                'TD', # TD Ameritrade brokerage\n",
    "                'RYAN', # Ryan Cohen, CEO of GME\n",
    "                'NYC', # New York City\n",
    "                'REG', # reg SHO \n",
    "                'SHO', # reg SHO \n",
    "                'NEXT', # common\n",
    "                'FREE', # spam\n",
    "                'DM', # direct message\n",
    "                'TV', # television\n",
    "                'ENS', # ethereum name service, spam\n",
    "                'IRS', # internal revenue service\n",
    "                'PR', # public relations\n",
    "                'IQ', # intelligence quotient\n",
    "                'VS', # versus\n",
    "                'PT', # price target\n",
    "                'IBKR', # interactive brokers\n",
    "                'GOOD', # common\n",
    "                'OPEN', # market open\n",
    "                'FCF', # free cash flow\n",
    "        \n",
    "                 \n",
    "                }\n",
    "    \n",
    "    combined_blacklist = set(blacklist) | set(word.upper() for word in stopwords.words('english'))\n",
    "       \n",
    "    \n",
    "    def find_tickers(text, compiled_pattern, tickers_set, blacklist_set):\n",
    "        # Find all matches\n",
    "        potential_tickers = compiled_pattern.findall(text)\n",
    "        # Filter matches against the tickers list and ensure they are not in the blacklist\n",
    "        return list(set([ticker for ticker in potential_tickers if ticker in tickers_set and ticker not in combined_blacklist]))\n",
    "\n",
    "    try:\n",
    "        df['title_tickers'] = df['title'].apply(lambda x: find_tickers(x, compiled_pattern, tickers_set, combined_blacklist))\n",
    "    except KeyError:\n",
    "        print('title not found, working with comments?')\n",
    "        \n",
    "    \n",
    "    df['selftext_tickers'] = df['selftext'].apply(lambda x: find_tickers(x, compiled_pattern, tickers_set, combined_blacklist))\n",
    "    \n",
    "    df['tickers'] = [list(set(x + y)) for x, y in zip(df['title_tickers'], df['selftext_tickers'])]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee89477-cfaa-4dba-bf08-fc2058866b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vader_sentiment(df):\n",
    "    \n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "    added_words = {\n",
    "            'citron': -4.0,  \n",
    "            'hidenburg': -4.0,        \n",
    "            'moon': 4.0,\n",
    "            'highs': 2.0,\n",
    "            'mooning': 4.0,\n",
    "            'long': 2.0,\n",
    "            'short': -2.0,\n",
    "            'call': 4.0,\n",
    "            'calls': 4.0,    \n",
    "            'put': -4.0,\n",
    "            'puts': -4.0,    \n",
    "            'break': 2.0,\n",
    "            'tendie': 2.0,\n",
    "            'tendies': 2.0,\n",
    "            'town': 2.0,     \n",
    "            'overvalued': -3.0,\n",
    "            'undervalued': 3.0,\n",
    "            'buy': 4.0,\n",
    "            'sell': -4.0,\n",
    "            'gone': -1.0,\n",
    "            'gtfo': -1.7,\n",
    "            'paper': -1.7,\n",
    "            'bullish': 3.7,\n",
    "            'bearish': -3.7,\n",
    "            'bagholder': -1.7,\n",
    "            'stonk': 1.9,\n",
    "            'green': 1.9,\n",
    "            'money': 1.2,\n",
    "            'print': 2.2,\n",
    "            'rocket': 2.2,\n",
    "            'bull': 2.9,\n",
    "            'bear': -2.9,\n",
    "            'pumping': -1.0,\n",
    "            'sus': -3.0,\n",
    "            'offering': -2.3,\n",
    "            'rip': -4.0,\n",
    "            'downgrade': -3.0,\n",
    "            'upgrade': 3.0,     \n",
    "            'maintain': 1.0,          \n",
    "            'pump': 1.9,\n",
    "            'hot': 1.5,\n",
    "            'drop': -2.5,\n",
    "            'rebound': 1.5,  \n",
    "            'crack': 2.5,\n",
    "            'ðŸš€': 3, # Jake ADDED THESE\n",
    "            'ðŸŒ•': 3, # Jake ADDED THESE\n",
    "            'YOLO': 4, # Jake ADDED THESE\n",
    "            'ripping': 3,# Jake ADDED THESE\n",
    "            'regarded': 0, # Jake ADDED THESE\n",
    "            'squeeze':3, # Jake ADDED THESE\n",
    "            }\n",
    "    \n",
    "    vader.lexicon.update(added_words)\n",
    "\n",
    "    def safe_sentiment(text):\n",
    "        try:\n",
    "            # Ensure the input is a non-empty string\n",
    "            if not isinstance(text, str) or not text.strip():\n",
    "                return 0\n",
    "\n",
    "            # Analyze the sentiment\n",
    "            sentiment_dict = vader.polarity_scores(text)\n",
    "            return sentiment_dict.get('compound', 0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: '{text}' (type: {type(text)}). Error: {e}\")\n",
    "            return 0\n",
    "\n",
    "    # Apply the function\n",
    "    try:\n",
    "        df['title_sentiment'] = df['title'].apply(safe_sentiment)\n",
    "    except:\n",
    "        print('Titles not found, is this a comments file?')\n",
    "        df['title_sentiment'] = 0\n",
    "        \n",
    "    df['selftext_sentiment'] = df['selftext'].apply(safe_sentiment)\n",
    "    \n",
    "    \n",
    "    def non_zero_average(row):\n",
    "        sentiments = [row['title_sentiment'], row['selftext_sentiment']]\n",
    "        non_zero_sentiments = [s for s in sentiments if s != 0]\n",
    "\n",
    "        if not non_zero_sentiments:\n",
    "            return 0  # Return 0 if both sentiments are zero\n",
    "\n",
    "        return sum(non_zero_sentiments) / len(non_zero_sentiments)\n",
    "\n",
    "    # Apply the function to calculate overall sentiment\n",
    "    df['overall_sentiment'] = df.apply(non_zero_average, axis=1)\n",
    "    \n",
    "    df['score_weighted_sentiment'] = df['overall_sentiment'] * df['score']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c963c5a-dfd5-4cd6-a32a-1c3f0b75e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_praw_submissions(limit):\n",
    "    \n",
    "    client_id = 'aWEYVIaAoJGlCPja3awh0A'\n",
    "    secret = 'gOR5FfkvsTH3MJ0IHRSImToTwt0PSQ'\n",
    "\n",
    "    reddit = praw.Reddit(\n",
    "                        client_id=client_id,\n",
    "                        client_secret=secret,\n",
    "                        user_agent=\"MADS/0.1 by TeamSafari\",\n",
    "                    )\n",
    "    submissions_data = []\n",
    "\n",
    "    for submission in reddit.subreddit(\"wallstreetbets\").new(limit=limit):\n",
    "        # print(dir(submission))\n",
    "        data = {\n",
    "            'id': submission.id,\n",
    "            'subreddit_id': submission.subreddit_id,\n",
    "            'subreddit': submission.subreddit,\n",
    "            'author': submission.author,\n",
    "            'created_utc': submission.created_utc,\n",
    "            'permalink': submission.permalink,\n",
    "            'title': submission.title,\n",
    "            'selftext': submission.selftext,\n",
    "            'num_comments': submission.num_comments,\n",
    "            'score': submission.score,\n",
    "            'flair': submission.link_flair_text,\n",
    "            'removal_reason':submission.removal_reason,\n",
    "            \n",
    "\n",
    "            # Add more fields as needed\n",
    "        }\n",
    "        submissions_data.append(data)\n",
    "\n",
    "    df = pd.DataFrame(submissions_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3dbaff8-ff32-4aeb-b8d4-3c2da23bd08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_reddit_praw_submissions(limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74e40f67-722c-42ad-8471-70664599f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "praw_df = preprocess_df(df)\n",
    "praw_df = find_tickers(praw_df)\n",
    "praw_df = add_vader_sentiment(praw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5f36583-930d-4305-925c-78811ffac508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>flair</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>created_EST_date</th>\n",
       "      <th>title_tickers</th>\n",
       "      <th>selftext_tickers</th>\n",
       "      <th>tickers</th>\n",
       "      <th>title_sentiment</th>\n",
       "      <th>selftext_sentiment</th>\n",
       "      <th>overall_sentiment</th>\n",
       "      <th>score_weighted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18enqaz</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Ok-Atmosphere-6272</td>\n",
       "      <td>1.702158e+09</td>\n",
       "      <td>/r/wallstreetbets/comments/18enqaz/what_is_goi...</td>\n",
       "      <td>What is going on with BLUE?</td>\n",
       "      <td>Just curious what peoples thoughts are about t...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>[BLUE]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[BLUE]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8305</td>\n",
       "      <td>0.83050</td>\n",
       "      <td>2.4915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18enliw</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>FunnyGagYT</td>\n",
       "      <td>1.702157e+09</td>\n",
       "      <td>/r/wallstreetbets/comments/18enliw/eu_ai_act_e...</td>\n",
       "      <td>EU AI Act, EU's New Landmark AI Act: What Does...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>News</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.07720</td>\n",
       "      <td>0.2316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18enepz</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Dull-Menu-5023</td>\n",
       "      <td>1.702157e+09</td>\n",
       "      <td>/r/wallstreetbets/comments/18enepz/anyone_feel...</td>\n",
       "      <td>Anyone feel like its 2021 all over again?</td>\n",
       "      <td>Both the stock market and crypto are rapidly c...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.30560</td>\n",
       "      <td>0.6112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18enbtj</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>realstocknear</td>\n",
       "      <td>1.702157e+09</td>\n",
       "      <td>/r/wallstreetbets/comments/18enbtj/created_a_f...</td>\n",
       "      <td>Created a free website that simplifies stock a...</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>Chart</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.6486</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.64860</td>\n",
       "      <td>4.5402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18emba6</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Naive-Historian-2110</td>\n",
       "      <td>1.702154e+09</td>\n",
       "      <td>/r/wallstreetbets/comments/18emba6/if_my_autis...</td>\n",
       "      <td>If my autism is correct SPY will crash...</td>\n",
       "      <td>x200B;\\n\\nIf declining interest in real estate...</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>Meme</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>-0.16235</td>\n",
       "      <td>-2.9223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>180o7ck</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>LasisuKibiras</td>\n",
       "      <td>1.700593e+09</td>\n",
       "      <td>/r/wallstreetbets/comments/180o7ck/gay_bears_g...</td>\n",
       "      <td>Gay bears get fcked</td>\n",
       "      <td>We can see a new pattern forming up that will ...</td>\n",
       "      <td>13</td>\n",
       "      <td>43</td>\n",
       "      <td>Meme</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.57190</td>\n",
       "      <td>24.5917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>180o13a</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>steveneverforgotme</td>\n",
       "      <td>1.700592e+09</td>\n",
       "      <td>/r/wallstreetbets/comments/180o13a/100_green_d...</td>\n",
       "      <td>100 green days from here on out</td>\n",
       "      <td>Finally found a good strategy with a 100 succe...</td>\n",
       "      <td>11</td>\n",
       "      <td>170</td>\n",
       "      <td>Meme</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>0.68560</td>\n",
       "      <td>116.5520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>180npxe</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>realstocknear</td>\n",
       "      <td>1.700591e+09</td>\n",
       "      <td>/r/wallstreetbets/comments/180npxe/how_things_...</td>\n",
       "      <td>How things are changing just in 1 Week</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>219</td>\n",
       "      <td>Meme</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>180nm1q</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>realstocknear</td>\n",
       "      <td>1.700591e+09</td>\n",
       "      <td>/r/wallstreetbets/comments/180nm1q/just_when_w...</td>\n",
       "      <td>just when we thought this week couldn't get an...</td>\n",
       "      <td></td>\n",
       "      <td>1200</td>\n",
       "      <td>9434</td>\n",
       "      <td>News</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.01910</td>\n",
       "      <td>180.1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>180n1kr</td>\n",
       "      <td>t5_2th52</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>goodtilcancelled</td>\n",
       "      <td>1.700589e+09</td>\n",
       "      <td>/r/wallstreetbets/comments/180n1kr/overlooked_...</td>\n",
       "      <td>Overlooked Recession Indicator</td>\n",
       "      <td>Everyone focuses on economic and technical ind...</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>News</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>-0.9664</td>\n",
       "      <td>-0.70340</td>\n",
       "      <td>-8.4408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>898 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id subreddit_id       subreddit                author   created_utc  \\\n",
       "0    18enqaz     t5_2th52  wallstreetbets    Ok-Atmosphere-6272  1.702158e+09   \n",
       "1    18enliw     t5_2th52  wallstreetbets            FunnyGagYT  1.702157e+09   \n",
       "2    18enepz     t5_2th52  wallstreetbets        Dull-Menu-5023  1.702157e+09   \n",
       "3    18enbtj     t5_2th52  wallstreetbets         realstocknear  1.702157e+09   \n",
       "4    18emba6     t5_2th52  wallstreetbets  Naive-Historian-2110  1.702154e+09   \n",
       "..       ...          ...             ...                   ...           ...   \n",
       "894  180o7ck     t5_2th52  wallstreetbets         LasisuKibiras  1.700593e+09   \n",
       "895  180o13a     t5_2th52  wallstreetbets    steveneverforgotme  1.700592e+09   \n",
       "896  180npxe     t5_2th52  wallstreetbets         realstocknear  1.700591e+09   \n",
       "897  180nm1q     t5_2th52  wallstreetbets         realstocknear  1.700591e+09   \n",
       "898  180n1kr     t5_2th52  wallstreetbets      goodtilcancelled  1.700589e+09   \n",
       "\n",
       "                                             permalink  \\\n",
       "0    /r/wallstreetbets/comments/18enqaz/what_is_goi...   \n",
       "1    /r/wallstreetbets/comments/18enliw/eu_ai_act_e...   \n",
       "2    /r/wallstreetbets/comments/18enepz/anyone_feel...   \n",
       "3    /r/wallstreetbets/comments/18enbtj/created_a_f...   \n",
       "4    /r/wallstreetbets/comments/18emba6/if_my_autis...   \n",
       "..                                                 ...   \n",
       "894  /r/wallstreetbets/comments/180o7ck/gay_bears_g...   \n",
       "895  /r/wallstreetbets/comments/180o13a/100_green_d...   \n",
       "896  /r/wallstreetbets/comments/180npxe/how_things_...   \n",
       "897  /r/wallstreetbets/comments/180nm1q/just_when_w...   \n",
       "898  /r/wallstreetbets/comments/180n1kr/overlooked_...   \n",
       "\n",
       "                                                 title  \\\n",
       "0                          What is going on with BLUE?   \n",
       "1    EU AI Act, EU's New Landmark AI Act: What Does...   \n",
       "2            Anyone feel like its 2021 all over again?   \n",
       "3    Created a free website that simplifies stock a...   \n",
       "4            If my autism is correct SPY will crash...   \n",
       "..                                                 ...   \n",
       "894                                Gay bears get fcked   \n",
       "895                    100 green days from here on out   \n",
       "896             How things are changing just in 1 Week   \n",
       "897  just when we thought this week couldn't get an...   \n",
       "898                     Overlooked Recession Indicator   \n",
       "\n",
       "                                              selftext  num_comments  score  \\\n",
       "0    Just curious what peoples thoughts are about t...             3      3   \n",
       "1                                                                  1      3   \n",
       "2    Both the stock market and crypto are rapidly c...             9      2   \n",
       "3                                                                  5      7   \n",
       "4    x200B;\\n\\nIf declining interest in real estate...            22     18   \n",
       "..                                                 ...           ...    ...   \n",
       "894  We can see a new pattern forming up that will ...            13     43   \n",
       "895  Finally found a good strategy with a 100 succe...            11    170   \n",
       "896                                                                4    219   \n",
       "897                                                             1200   9434   \n",
       "898  Everyone focuses on economic and technical ind...             8     12   \n",
       "\n",
       "          flair removal_reason created_EST_date title_tickers  \\\n",
       "0    Discussion           None       2023-12-09        [BLUE]   \n",
       "1          News           None       2023-12-09            []   \n",
       "2    Discussion           None       2023-12-09            []   \n",
       "3         Chart           None       2023-12-09            []   \n",
       "4          Meme           None       2023-12-09            []   \n",
       "..          ...            ...              ...           ...   \n",
       "894        Meme           None       2023-11-21            []   \n",
       "895        Meme           None       2023-11-21            []   \n",
       "896        Meme           None       2023-11-21            []   \n",
       "897        News           None       2023-11-21            []   \n",
       "898        News           None       2023-11-21            []   \n",
       "\n",
       "    selftext_tickers tickers  title_sentiment  selftext_sentiment  \\\n",
       "0                 []  [BLUE]           0.0000              0.8305   \n",
       "1                 []      []           0.0772              0.0000   \n",
       "2                 []      []           0.3612              0.2500   \n",
       "3                 []      []           0.6486              0.0000   \n",
       "4                 []      []          -0.4019              0.0772   \n",
       "..               ...     ...              ...                 ...   \n",
       "894               []      []           0.0000              0.5719   \n",
       "895               []      []           0.4404              0.9308   \n",
       "896               []      []           0.0000              0.0000   \n",
       "897               []      []           0.0191              0.0000   \n",
       "898               []      []          -0.4404             -0.9664   \n",
       "\n",
       "     overall_sentiment  score_weighted_sentiment  \n",
       "0              0.83050                    2.4915  \n",
       "1              0.07720                    0.2316  \n",
       "2              0.30560                    0.6112  \n",
       "3              0.64860                    4.5402  \n",
       "4             -0.16235                   -2.9223  \n",
       "..                 ...                       ...  \n",
       "894            0.57190                   24.5917  \n",
       "895            0.68560                  116.5520  \n",
       "896            0.00000                    0.0000  \n",
       "897            0.01910                  180.1894  \n",
       "898           -0.70340                   -8.4408  \n",
       "\n",
       "[898 rows x 20 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "praw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e725964a-dc84-4ef6-b48c-86cd9868f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cumulative_sentiment_sorted_df\n",
    "exploded_df = praw_df.explode('tickers')\n",
    "cumulative_sentiment = exploded_df.groupby('tickers')['overall_sentiment'].sum().reset_index() # Group by 'tickers'\n",
    "cumulative_sentiment.columns = ['Ticker', 'Cumulative Overall Sentiment'] # Rename columns for clarity\n",
    "cumulative_sentiment_sorted = cumulative_sentiment.sort_values(by='Cumulative Overall Sentiment', ascending=False)\n",
    "# display(cumulative_sentiment_sorted)\n",
    "\n",
    "# Group by 'tickers' and sum the 'score_weighted_sentiment'\n",
    "cumulative_weighted_sentiment = exploded_df.groupby('tickers')['score_weighted_sentiment'].sum().reset_index()\n",
    "cumulative_weighted_sentiment.columns = ['Ticker', 'Cumulative Weighted Sentiment'] # Rename columns for clarity\n",
    "cumulative_weighted_sentiment_sorted = cumulative_weighted_sentiment.sort_values(by='Cumulative Weighted Sentiment', ascending=False)\n",
    "cumulative_weighted_sentiment_sorted['Date'] = praw_df['created_EST_date']\n",
    "cumulative_weighted_sentiment_sorted['Date'].fillna(praw_df['created_EST_date'].unique()[0],inplace=True)\n",
    "# display(cumulative_weighted_sentiment_sorted)\n",
    "\n",
    "daily_sentiment_df = cumulative_sentiment_sorted.merge(cumulative_weighted_sentiment_sorted, on='Ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a281a52-a23d-4c37-9d65-927a29cc12ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Cumulative Overall Sentiment</th>\n",
       "      <th>Cumulative Weighted Sentiment</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAVE</td>\n",
       "      <td>8.10645</td>\n",
       "      <td>310.38380</td>\n",
       "      <td>2023-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>7.58940</td>\n",
       "      <td>4020.85095</td>\n",
       "      <td>2023-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>META</td>\n",
       "      <td>5.32785</td>\n",
       "      <td>6872.84705</td>\n",
       "      <td>2023-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>4.24275</td>\n",
       "      <td>807.51960</td>\n",
       "      <td>2023-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMD</td>\n",
       "      <td>4.04835</td>\n",
       "      <td>-292.11000</td>\n",
       "      <td>2023-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>PAM</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-7.60880</td>\n",
       "      <td>2023-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>SUPV</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-7.60880</td>\n",
       "      <td>2023-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>BBVA</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-7.60880</td>\n",
       "      <td>2023-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>YPF</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-7.60880</td>\n",
       "      <td>2023-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>DHI</td>\n",
       "      <td>-1.30180</td>\n",
       "      <td>-45.01100</td>\n",
       "      <td>2023-12-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ticker  Cumulative Overall Sentiment  Cumulative Weighted Sentiment  \\\n",
       "0     SAVE                       8.10645                      310.38380   \n",
       "1     NVDA                       7.58940                     4020.85095   \n",
       "2     META                       5.32785                     6872.84705   \n",
       "3     MSFT                       4.24275                      807.51960   \n",
       "4      AMD                       4.04835                     -292.11000   \n",
       "..     ...                           ...                            ...   \n",
       "191    PAM                      -0.95110                       -7.60880   \n",
       "192   SUPV                      -0.95110                       -7.60880   \n",
       "193   BBVA                      -0.95110                       -7.60880   \n",
       "194    YPF                      -0.95110                       -7.60880   \n",
       "195    DHI                      -1.30180                      -45.01100   \n",
       "\n",
       "           Date  \n",
       "0    2023-12-07  \n",
       "1    2023-12-07  \n",
       "2    2023-12-07  \n",
       "3    2023-12-07  \n",
       "4    2023-12-09  \n",
       "..          ...  \n",
       "191  2023-12-07  \n",
       "192  2023-12-06  \n",
       "193  2023-12-09  \n",
       "194  2023-12-06  \n",
       "195  2023-12-08  \n",
       "\n",
       "[196 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca6c7ff9-df16-4718-84d8-0d9e8106ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the DataFrame if 'tickers' column contains lists\n",
    "exploded_df = praw_df.explode('tickers')\n",
    "\n",
    "expected_columns = ['tickers', 'title', 'permalink', 'overall_sentiment']\n",
    "\n",
    "# Check if all expected columns are in the DataFrame\n",
    "if not all(column in exploded_df.columns for column in expected_columns):\n",
    "    raise ValueError(\"DataFrame does not contain the expected columns.\")\n",
    "\n",
    "# Aggregate submission details\n",
    "submission_details = exploded_df.groupby('tickers').apply(\n",
    "    lambda x: [(title, permalink, sentiment) \n",
    "               for title, permalink, sentiment in zip(x['title'], x['permalink'], x['overall_sentiment'])]\n",
    ").reset_index(name='Submissions')\n",
    "\n",
    "# Rename columns for clarity\n",
    "submission_details.columns = ['Ticker', 'Submissions']\n",
    "\n",
    "# Merge with daily_sentiment_df\n",
    "daily_sentiment_df = daily_sentiment_df.merge(submission_details, on='Ticker', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ee6a74d-fbfa-4e05-a37c-39ba91a6f3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Cumulative Overall Sentiment</th>\n",
       "      <th>Cumulative Weighted Sentiment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Submissions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAVE</td>\n",
       "      <td>8.10645</td>\n",
       "      <td>310.38380</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>[(SAVE merger question, /r/wallstreetbets/comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>7.58940</td>\n",
       "      <td>4020.85095</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>[(AMD AI Chip release - Up 6.5 in the last 24h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>META</td>\n",
       "      <td>5.32785</td>\n",
       "      <td>6872.84705</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>[(The guy who went full send on META calls sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>4.24275</td>\n",
       "      <td>807.51960</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>[(Trying to recoup my loss: Day 1, /r/wallstre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMD</td>\n",
       "      <td>4.04835</td>\n",
       "      <td>-292.11000</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>[(Thank you Lisa Su  AMD Gainz, /r/wallstreetb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>PAM</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-7.60880</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>[(Shorting select Argentinian companies, /r/wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>SUPV</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-7.60880</td>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>[(Shorting select Argentinian companies, /r/wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>BBVA</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-7.60880</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>[(Shorting select Argentinian companies, /r/wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>YPF</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-7.60880</td>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>[(Shorting select Argentinian companies, /r/wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>DHI</td>\n",
       "      <td>-1.30180</td>\n",
       "      <td>-45.01100</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>[(Did Warren Buffett make a mistake? BUY DHI P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ticker  Cumulative Overall Sentiment  Cumulative Weighted Sentiment  \\\n",
       "0     SAVE                       8.10645                      310.38380   \n",
       "1     NVDA                       7.58940                     4020.85095   \n",
       "2     META                       5.32785                     6872.84705   \n",
       "3     MSFT                       4.24275                      807.51960   \n",
       "4      AMD                       4.04835                     -292.11000   \n",
       "..     ...                           ...                            ...   \n",
       "191    PAM                      -0.95110                       -7.60880   \n",
       "192   SUPV                      -0.95110                       -7.60880   \n",
       "193   BBVA                      -0.95110                       -7.60880   \n",
       "194    YPF                      -0.95110                       -7.60880   \n",
       "195    DHI                      -1.30180                      -45.01100   \n",
       "\n",
       "           Date                                        Submissions  \n",
       "0    2023-12-07  [(SAVE merger question, /r/wallstreetbets/comm...  \n",
       "1    2023-12-07  [(AMD AI Chip release - Up 6.5 in the last 24h...  \n",
       "2    2023-12-07  [(The guy who went full send on META calls sho...  \n",
       "3    2023-12-07  [(Trying to recoup my loss: Day 1, /r/wallstre...  \n",
       "4    2023-12-09  [(Thank you Lisa Su  AMD Gainz, /r/wallstreetb...  \n",
       "..          ...                                                ...  \n",
       "191  2023-12-07  [(Shorting select Argentinian companies, /r/wa...  \n",
       "192  2023-12-06  [(Shorting select Argentinian companies, /r/wa...  \n",
       "193  2023-12-09  [(Shorting select Argentinian companies, /r/wa...  \n",
       "194  2023-12-06  [(Shorting select Argentinian companies, /r/wa...  \n",
       "195  2023-12-08  [(Did Warren Buffett make a mistake? BUY DHI P...  \n",
       "\n",
       "[196 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad05357-783f-4b6b-b755-3fe12f929115",
   "metadata": {},
   "source": [
    "## For use in dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a67d3eb-c2aa-4890-91cb-ae58cc98ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import praw\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b989b70-ad87-40ec-9e86-5ecf29abf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRAW_for_dashboard():\n",
    "    def preprocess_df(df):\n",
    "        def preprocess_text(text):\n",
    "\n",
    "            # Remove specific unwanted characters\n",
    "            text = re.sub(r'[^A-Za-z0-9\\s,.!?;:()\\'\\\"-]', '', text)\n",
    "\n",
    "            # Strip whitespace\n",
    "            text = text.strip()\n",
    "\n",
    "            return text\n",
    "\n",
    "        try:\n",
    "            df['title'] = df['title'].fillna('')\n",
    "            df['title'] = df['title'].apply(preprocess_text)\n",
    "\n",
    "            # Create a new column with shifted values\n",
    "            df['title_prior'] = df['title'].shift(1)\n",
    "            # Drop rows where the value in 'ColumnToCheck' is the same as in 'ShiftedColumn'\n",
    "            df = df[df['title'] != df['title_prior']]\n",
    "            # drop the 'ShiftedColumn'\n",
    "            df = df.drop('title_prior', axis=1)\n",
    "\n",
    "        except:\n",
    "            print(f'No title found, skipping')\n",
    "\n",
    "\n",
    "        # handle blank \n",
    "        df['selftext'] = df['selftext'].fillna('') \n",
    "\n",
    "        # preprocess selftext\n",
    "        df['selftext'] = df['selftext'].apply(preprocess_text)\n",
    "\n",
    "        # localize the UTC time stamp\n",
    "        df['created_EST_date'] = pd.to_datetime(df['created_utc'], unit='s').dt.tz_localize('UTC').dt.tz_convert('America/New_York').dt.date\n",
    "\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def find_tickers(df):\n",
    " \n",
    "        # Load tickers from a CSV file\n",
    "        # stocks = pd.read_csv(r'/root/Git Repo/SIADS_Capstone_Group17/Data/nasdaq_screener.csv')\n",
    "        stocks = pd.read_csv(r'./Data/nasdaq_screener.csv')\n",
    "\n",
    "        # Directly convert the 'Symbol' column to a set\n",
    "        tickers_set = set(stocks['Symbol'])\n",
    "\n",
    "        # Use set union to add additional tickers\n",
    "        additional_tickers = {'BBBY'}\n",
    "        tickers_set = tickers_set.union(additional_tickers)\n",
    "\n",
    "        # Adjust the pattern to optionally include a leading '$'\n",
    "        # pattern = r'\\b\\$?(?:' + '|'.join(tickers_as_strings) + r')\\b'\n",
    "        # pattern = r'\\b\\$?(?:\\(?)(?:' + '|'.join(tickers_as_strings) + r')(?:\\)?)\\b'\n",
    "        pattern = r'\\b[A-Z]{2,5}\\b'\n",
    "        compiled_pattern = re.compile(pattern)\n",
    "\n",
    "\n",
    "\n",
    "        blacklist = {\n",
    "    #                 {'I', 'ELON', 'WSB', 'THE', 'A', 'ROPE', 'YOLO', 'TOS', 'CEO', 'DD', 'IT', 'OPEN', 'ATH', 'PM', 'IRS', 'FOR',\n",
    "    #              'DEC', 'BE',\n",
    "                    'IMO',# 'ALL', 'RH', 'EV', 'TOS', 'CFO', 'CTO',\n",
    "                    'DD',\n",
    "                    #'BTFD', 'WSB', 'OK', 'PDT', 'RH', 'KYS', 'FD',\n",
    "    #              'TYS', \n",
    "                    'US',\n",
    "                    'USA',\n",
    "                    # 'IT', 'ATH', 'RIP', 'BMW', 'GDP', 'OTM', 'ATM', 'ITM', \n",
    "                    'IMO', 'LOL', 'AM', 'BE', 'PR', 'PRAY',\n",
    "    #              'PT', 'FBI', 'SEC', 'GOD', 'NOT', 'POS', 'FOMO', 'TL;DR',\n",
    "                    'EDIT', 'STILL', 'WTF', 'RAW', 'PM', 'LMAO', 'LMFAO',\n",
    "    #              'ROFL', 'EZ', 'RED', 'BEZOS', 'TICK', 'IS', 'PM', 'LPT', 'GOAT', 'FL', 'CA', 'IL', 'MACD', 'HQ', 'OP', 'PS', 'AH',\n",
    "    #              'TL', 'JAN', 'FEB', 'JUL', 'AUG', 'SEP', 'SEPT', 'OCT', 'NOV', 'FDA', 'IV', 'ER', 'IPO', 'MILF', 'BUT', 'SSN', 'FIFA',\n",
    "    #              'USD', 'CPU', 'AT', 'GG', 'Mar', \n",
    "\n",
    "    #                # Jake added\n",
    "                    'RUN', # common\n",
    "                    'SAY', # common\n",
    "                    'EOD', # end of day\n",
    "                    'BIG', # common\n",
    "                    'LOW', # low / high\n",
    "                    'RSI', #relative strenght\n",
    "                    'DT', #double top\n",
    "                    'HUGE',\n",
    "                    'U', # you\n",
    "                    'AI', # Artificial Intelligence\n",
    "                    'DC', # washington DC\n",
    "                    'J', # as in J Powell\n",
    "                    'ES', # E-mini SP future\n",
    "                    'F', # f*ck\n",
    "                    'GO',\n",
    "                    'UK', # United Kingdom\n",
    "                    'EU', # european union\n",
    "                    'RH', # Robinhood, not Restoration Hardware\n",
    "                    'E', # E*trade brokerage\n",
    "                    'L', # L for loss, P&L etc\n",
    "                    'R', # common \n",
    "                    'K', # OK\n",
    "                    'B', # common in BBBY odd spacing (spam?)\n",
    "                    'TD', # TD Ameritrade brokerage\n",
    "                    'RYAN', # Ryan Cohen, CEO of GME\n",
    "                    'NYC', # New York City\n",
    "                    'REG', # reg SHO \n",
    "                    'SHO', # reg SHO \n",
    "                    'NEXT', # common\n",
    "                    'FREE', # spam\n",
    "                    'DM', # direct message\n",
    "                    'TV', # television\n",
    "                    'ENS', # ethereum name service, spam\n",
    "                    'IRS', # internal revenue service\n",
    "                    'PR', # public relations\n",
    "                    'IQ', # intelligence quotient\n",
    "                    'VS', # versus\n",
    "                    'PT', # price target\n",
    "                    'IBKR', # interactive brokers\n",
    "                    'GOOD', # common\n",
    "                    'OPEN', # market open\n",
    "                    'FCF', # free cash flow\n",
    "\n",
    "\n",
    "                    }\n",
    "\n",
    "        combined_blacklist = set(blacklist) | set(word.upper() for word in stopwords.words('english'))\n",
    "\n",
    "\n",
    "        def find_tickers(text, compiled_pattern, tickers_set, blacklist_set):\n",
    "            # Find all matches\n",
    "            potential_tickers = compiled_pattern.findall(text)\n",
    "            # Filter matches against the tickers list and ensure they are not in the blacklist\n",
    "            return list(set([ticker for ticker in potential_tickers if ticker in tickers_set and ticker not in combined_blacklist]))\n",
    "\n",
    "        try:\n",
    "            df['title_tickers'] = df['title'].apply(lambda x: find_tickers(x, compiled_pattern, tickers_set, combined_blacklist))\n",
    "        except KeyError:\n",
    "            print('title not found, working with comments?')\n",
    "\n",
    "        df['selftext_tickers'] = df['selftext'].apply(lambda x: find_tickers(x, compiled_pattern, tickers_set, combined_blacklist))\n",
    "        df['tickers'] = [list(set(x + y)) for x, y in zip(df['title_tickers'], df['selftext_tickers'])]\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def add_vader_sentiment(df):\n",
    "\n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "        added_words = {\n",
    "                'citron': -4.0,  \n",
    "                'hidenburg': -4.0,        \n",
    "                'moon': 4.0,\n",
    "                'highs': 2.0,\n",
    "                'mooning': 4.0,\n",
    "                'long': 2.0,\n",
    "                'short': -2.0,\n",
    "                'call': 4.0,\n",
    "                'calls': 4.0,    \n",
    "                'put': -4.0,\n",
    "                'puts': -4.0,    \n",
    "                'break': 2.0,\n",
    "                'tendie': 2.0,\n",
    "                'tendies': 2.0,\n",
    "                'town': 2.0,     \n",
    "                'overvalued': -3.0,\n",
    "                'undervalued': 3.0,\n",
    "                'buy': 4.0,\n",
    "                'sell': -4.0,\n",
    "                'gone': -1.0,\n",
    "                'gtfo': -1.7,\n",
    "                'paper': -1.7,\n",
    "                'bullish': 3.7,\n",
    "                'bearish': -3.7,\n",
    "                'bagholder': -1.7,\n",
    "                'stonk': 1.9,\n",
    "                'green': 1.9,\n",
    "                'money': 1.2,\n",
    "                'print': 2.2,\n",
    "                'rocket': 2.2,\n",
    "                'bull': 2.9,\n",
    "                'bear': -2.9,\n",
    "                'pumping': -1.0,\n",
    "                'sus': -3.0,\n",
    "                'offering': -2.3,\n",
    "                'rip': -4.0,\n",
    "                'downgrade': -3.0,\n",
    "                'upgrade': 3.0,     \n",
    "                'maintain': 1.0,          \n",
    "                'pump': 1.9,\n",
    "                'hot': 1.5,\n",
    "                'drop': -2.5,\n",
    "                'rebound': 1.5,  \n",
    "                'crack': 2.5,\n",
    "                'ðŸš€': 3, # Jake ADDED THESE\n",
    "                'ðŸŒ•': 3, # Jake ADDED THESE\n",
    "                'YOLO': 4, # Jake ADDED THESE\n",
    "                'ripping': 3,# Jake ADDED THESE\n",
    "                'regarded': 0, # Jake ADDED THESE\n",
    "                'squeeze':3, # Jake ADDED THESE\n",
    "                }\n",
    "\n",
    "        vader.lexicon.update(added_words)\n",
    "\n",
    "        def safe_sentiment(text):\n",
    "            try:\n",
    "                # Ensure the input is a non-empty string\n",
    "                if not isinstance(text, str) or not text.strip():\n",
    "                    return 0\n",
    "\n",
    "                # Analyze the sentiment\n",
    "                sentiment_dict = vader.polarity_scores(text)\n",
    "                return sentiment_dict.get('compound', 0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text: '{text}' (type: {type(text)}). Error: {e}\")\n",
    "                return 0\n",
    "\n",
    "        # Apply the function\n",
    "        try:\n",
    "            df['title_sentiment'] = df['title'].apply(safe_sentiment)\n",
    "        except:\n",
    "            print('Titles not found, is this a comments file?')\n",
    "            df['title_sentiment'] = 0\n",
    "\n",
    "        df['selftext_sentiment'] = df['selftext'].apply(safe_sentiment)\n",
    "\n",
    "\n",
    "        def non_zero_average(row):\n",
    "            sentiments = [row['title_sentiment'], row['selftext_sentiment']]\n",
    "            non_zero_sentiments = [s for s in sentiments if s != 0]\n",
    "\n",
    "            if not non_zero_sentiments:\n",
    "                return 0  # Return 0 if both sentiments are zero\n",
    "\n",
    "            return sum(non_zero_sentiments) / len(non_zero_sentiments)\n",
    "\n",
    "        # Apply the function to calculate overall sentiment\n",
    "        df['overall_sentiment'] = df.apply(non_zero_average, axis=1)\n",
    "\n",
    "        df['score_weighted_sentiment'] = df['overall_sentiment'] * df['score']\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def get_reddit_praw_submissions(limit):\n",
    "    \n",
    "        client_id = 'aWEYVIaAoJGlCPja3awh0A'\n",
    "        secret = 'gOR5FfkvsTH3MJ0IHRSImToTwt0PSQ'\n",
    "\n",
    "        reddit = praw.Reddit(\n",
    "                            client_id=client_id,\n",
    "                            client_secret=secret,\n",
    "                            user_agent=\"MADS/0.1 by TeamSafari\",\n",
    "                        )\n",
    "        submissions_data = []\n",
    "\n",
    "        for submission in reddit.subreddit(\"wallstreetbets\").new(limit=limit):\n",
    "            # print(dir(submission))\n",
    "            data = {\n",
    "                'id': submission.id,\n",
    "                'subreddit_id': submission.subreddit_id,\n",
    "                'subreddit': submission.subreddit,\n",
    "                'author': submission.author,\n",
    "                'created_utc': submission.created_utc,\n",
    "                'permalink': submission.permalink,\n",
    "                'title': submission.title,\n",
    "                'selftext': submission.selftext,\n",
    "                'num_comments': submission.num_comments,\n",
    "                'score': submission.score,\n",
    "                'flair': submission.link_flair_text,\n",
    "                'removal_reason':submission.removal_reason,\n",
    "\n",
    "\n",
    "                # Add more fields as needed\n",
    "            }\n",
    "            submissions_data.append(data)\n",
    "\n",
    "        df = pd.DataFrame(submissions_data)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    df = get_reddit_praw_submissions(limit=1000)\n",
    "    praw_df = preprocess_df(df)\n",
    "    praw_df = find_tickers(praw_df)\n",
    "    praw_df = add_vader_sentiment(praw_df)\n",
    "    \n",
    "    \n",
    "    # display(praw_df)\n",
    "    \n",
    "    \n",
    "    # create cumulative_sentiment_sorted_df\n",
    "    exploded_df = praw_df.explode('tickers')\n",
    "    cumulative_sentiment = exploded_df.groupby('tickers')['overall_sentiment'].sum().reset_index() # Group by 'tickers'\n",
    "    cumulative_sentiment.columns = ['Ticker', 'Cumulative Overall Sentiment'] # Rename columns for clarity\n",
    "    cumulative_sentiment_sorted = cumulative_sentiment.sort_values(by='Cumulative Overall Sentiment', ascending=False)\n",
    "    # display(cumulative_sentiment_sorted)\n",
    "\n",
    "    # Group by 'tickers' and sum the 'score_weighted_sentiment'\n",
    "    cumulative_weighted_sentiment = exploded_df.groupby('tickers')['score_weighted_sentiment'].sum().reset_index()\n",
    "    cumulative_weighted_sentiment.columns = ['Ticker', 'Cumulative Weighted Sentiment'] # Rename columns for clarity\n",
    "    cumulative_weighted_sentiment_sorted = cumulative_weighted_sentiment.sort_values(by='Cumulative Weighted Sentiment', ascending=False)\n",
    "    cumulative_weighted_sentiment_sorted['Date'] = praw_df['created_EST_date']\n",
    "    cumulative_weighted_sentiment_sorted['Date'].fillna(praw_df['created_EST_date'].unique()[0],inplace=True)\n",
    "    # display(cumulative_weighted_sentiment_sorted)\n",
    "\n",
    "    daily_sentiment_df = cumulative_sentiment_sorted.merge(cumulative_weighted_sentiment_sorted, on='Ticker')\n",
    "    \n",
    "    # Explode the DataFrame if 'tickers' column contains lists\n",
    "    # exploded_df = praw_df.explode('tickers')\n",
    "\n",
    "    expected_columns = ['tickers', 'title', 'permalink', 'overall_sentiment']\n",
    "\n",
    "    # Check if all expected columns are in the DataFrame\n",
    "    if not all(column in exploded_df.columns for column in expected_columns):\n",
    "        raise ValueError(\"DataFrame does not contain the expected columns.\")\n",
    "\n",
    "    # Base URL to prepend to permalinks\n",
    "    base_url = 'https://www.reddit.com/'\n",
    "    \n",
    "    # Aggregate submission details with the base URL prepended to permalinks\n",
    "    submission_details = exploded_df.groupby('tickers').apply(\n",
    "        lambda x: [(title, base_url + permalink, sentiment) \n",
    "                   for title, permalink, sentiment in zip(x['title'], x['permalink'], x['overall_sentiment'])]\n",
    "    ).reset_index(name='Submissions')\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    submission_details.columns = ['Ticker', 'Submissions']\n",
    "\n",
    "    # Merge with daily_sentiment_df\n",
    "    daily_sentiment_df = daily_sentiment_df.merge(submission_details, on='Ticker', how='left')\n",
    "    \n",
    "    return daily_sentiment_df, exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e107d3c-f680-4892-b008-09ae28ea6f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.89 s, sys: 0 ns, total: 1.89 s\n",
      "Wall time: 13.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Cumulative Overall Sentiment</th>\n",
       "      <th>Cumulative Weighted Sentiment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Submissions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAVE</td>\n",
       "      <td>8.89415</td>\n",
       "      <td>314.09580</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>[(Odds of the SAVE case being decided on or be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>6.59190</td>\n",
       "      <td>4004.81645</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>[(AMD AI Chip release - Up 6.5 in the last 24h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>META</td>\n",
       "      <td>5.32785</td>\n",
       "      <td>6937.98870</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>[(The guy who went full send on META calls sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>4.24275</td>\n",
       "      <td>807.29140</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>[(Trying to recoup my loss: Day 1, https://www...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>3.97220</td>\n",
       "      <td>2090.27530</td>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>[(Only took me 9 days to lose 290k yoloing on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>BMA</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-8.55990</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>[(Shorting select Argentinian companies, https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>BBVA</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-8.55990</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>[(Shorting select Argentinian companies, https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NOV</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-8.55990</td>\n",
       "      <td>2023-12-07</td>\n",
       "      <td>[(Shorting select Argentinian companies, https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>YPF</td>\n",
       "      <td>-0.95110</td>\n",
       "      <td>-8.55990</td>\n",
       "      <td>2023-12-06</td>\n",
       "      <td>[(Shorting select Argentinian companies, https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>DHI</td>\n",
       "      <td>-1.30180</td>\n",
       "      <td>-42.88580</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>[(Did Warren Buffett make a mistake? BUY DHI P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ticker  Cumulative Overall Sentiment  Cumulative Weighted Sentiment  \\\n",
       "0     SAVE                       8.89415                      314.09580   \n",
       "1     NVDA                       6.59190                     4004.81645   \n",
       "2     META                       5.32785                     6937.98870   \n",
       "3     MSFT                       4.24275                      807.29140   \n",
       "4     TSLA                       3.97220                     2090.27530   \n",
       "..     ...                           ...                            ...   \n",
       "194    BMA                      -0.95110                       -8.55990   \n",
       "195   BBVA                      -0.95110                       -8.55990   \n",
       "196    NOV                      -0.95110                       -8.55990   \n",
       "197    YPF                      -0.95110                       -8.55990   \n",
       "198    DHI                      -1.30180                      -42.88580   \n",
       "\n",
       "           Date                                        Submissions  \n",
       "0    2023-12-07  [(Odds of the SAVE case being decided on or be...  \n",
       "1    2023-12-07  [(AMD AI Chip release - Up 6.5 in the last 24h...  \n",
       "2    2023-12-08  [(The guy who went full send on META calls sho...  \n",
       "3    2023-12-07  [(Trying to recoup my loss: Day 1, https://www...  \n",
       "4    2023-12-06  [(Only took me 9 days to lose 290k yoloing on ...  \n",
       "..          ...                                                ...  \n",
       "194  2023-12-09  [(Shorting select Argentinian companies, https...  \n",
       "195  2023-12-09  [(Shorting select Argentinian companies, https...  \n",
       "196  2023-12-07  [(Shorting select Argentinian companies, https...  \n",
       "197  2023-12-06  [(Shorting select Argentinian companies, https...  \n",
       "198  2023-12-08  [(Did Warren Buffett make a mistake? BUY DHI P...  \n",
       "\n",
       "[199 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "output1, output2 = PRAW_for_dashboard()\n",
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20b86588-5c1b-4b24-96b6-9095af6059c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3 = output2[[\"tickers\", \"title\", \"permalink\", \"overall_sentiment\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d8fe1c1-8697-417c-9662-809cffa95573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Odds of the SAVE case being decided on or before January 19th?',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/18epb7p/odds_of_the_save_case_being_decided_on_or_before/',\n",
       "  0.7877000000000001),\n",
       " ('SAVE merger question',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/18cl5jf/save_merger_question/',\n",
       "  0.772),\n",
       " ('SAVE trial bullish quotes from the judge',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/18by8rd/save_trial_bullish_quotes_from_the_judge/',\n",
       "  0.89165),\n",
       " ('What happens to SAVE if JetBlue merger is blocked by DOJ?',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/18bjkra/what_happens_to_save_if_jetblue_merger_is_blocked/',\n",
       "  0.64795),\n",
       " ('What are your thoughts on the SpiritJetblue merger court case given the HawaiianAlaska deal?',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/18aq3j1/what_are_your_thoughts_on_the_spiritjetblue/',\n",
       "  0.7603),\n",
       " ('SAVE put assignment',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/189i3um/save_put_assignment/',\n",
       "  -0.5132),\n",
       " ('93K SAVE Merger YOLO',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/188h3xv/93k_save_merger_yolo/',\n",
       "  0.7761),\n",
       " ('SAVE BOX OR BUGATTI',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/186wt0v/save_box_or_bugatti/',\n",
       "  0.4939),\n",
       " ('SAVE Trial update',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/186jsmd/save_trial_update/',\n",
       "  0.79695),\n",
       " ('15k Gain so far from SAVE',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/186780a/15k_gain_so_far_from_save/',\n",
       "  0.6817),\n",
       " ('300k SAVE merger yolo update',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/1866lkr/300k_save_merger_yolo_update/',\n",
       "  0.7213),\n",
       " ('SAVE popping off (part deux)',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/1860ti5/save_popping_off_part_deux/',\n",
       "  0.24415),\n",
       " ('About the SAVE and JBLU merger',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/183emcn/about_the_save_and_jblu_merger/',\n",
       "  0.73875),\n",
       " ('SAVE Merger Risks',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/183axyw/save_merger_risks/',\n",
       "  0.2975),\n",
       " ('SAVE info',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/1814ug9/save_info/',\n",
       "  0.7974)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"Submissions\"].values[0].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d95c34e-a183-4a41-a2ed-8dc33b8d40bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Odds of the SAVE case being decided on or before January 19th?',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/18epb7p/odds_of_the_save_case_being_decided_on_or_before/',\n",
       "  0.7877000000000001),\n",
       " ('SAVE merger question',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/18cl5jf/save_merger_question/',\n",
       "  0.772),\n",
       " ('SAVE trial bullish quotes from the judge',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/18by8rd/save_trial_bullish_quotes_from_the_judge/',\n",
       "  0.89165),\n",
       " ('What happens to SAVE if JetBlue merger is blocked by DOJ?',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/18bjkra/what_happens_to_save_if_jetblue_merger_is_blocked/',\n",
       "  0.64795),\n",
       " ('What are your thoughts on the SpiritJetblue merger court case given the HawaiianAlaska deal?',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/18aq3j1/what_are_your_thoughts_on_the_spiritjetblue/',\n",
       "  0.7603),\n",
       " ('SAVE put assignment',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/189i3um/save_put_assignment/',\n",
       "  -0.5132),\n",
       " ('93K SAVE Merger YOLO',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/188h3xv/93k_save_merger_yolo/',\n",
       "  0.7761),\n",
       " ('SAVE BOX OR BUGATTI',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/186wt0v/save_box_or_bugatti/',\n",
       "  0.4939),\n",
       " ('SAVE Trial update',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/186jsmd/save_trial_update/',\n",
       "  0.79695),\n",
       " ('15k Gain so far from SAVE',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/186780a/15k_gain_so_far_from_save/',\n",
       "  0.6817),\n",
       " ('300k SAVE merger yolo update',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/1866lkr/300k_save_merger_yolo_update/',\n",
       "  0.7213),\n",
       " ('SAVE popping off (part deux)',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/1860ti5/save_popping_off_part_deux/',\n",
       "  0.24415),\n",
       " ('About the SAVE and JBLU merger',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/183emcn/about_the_save_and_jblu_merger/',\n",
       "  0.73875),\n",
       " ('SAVE Merger Risks',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/183axyw/save_merger_risks/',\n",
       "  0.2975),\n",
       " ('SAVE info',\n",
       "  'https://www.reddit.com//r/wallstreetbets/comments/1814ug9/save_info/',\n",
       "  0.7974)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.iloc[0,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7c26d-ff8f-4de5-895e-3799ad48e70a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-north-1:243637512696:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
